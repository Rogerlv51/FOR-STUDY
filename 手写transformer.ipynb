{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlfWax6btbbh"
      },
      "source": [
        "## D2L版本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N60lUQXas1mu"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DotProd_Attention(nn.Module):\n",
        "    def __init__(self, dropout, **kwargs) -> None:\n",
        "        super(DotProd_Attention, self).__init__(**kwargs)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, querys, keys, values, valid_lens=None):\n",
        "        d = querys.shape[-1]\n",
        "        # 设置transpose_b=True为了交换keys的最后两个维度\n",
        "        scores = torch.bmm(querys, keys.transpose(1, 2)) / math.sqrt(d)\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
        "    \n",
        "def masked_softmax(x, valid_lens):   # valid_lens指定我们要保留的有效长度，其余部分作掩码处理，置为一个很大的负数，可以用numpy中的inf\n",
        "    if valid_lens == None:    # 没指定的话，就直接softmax即可\n",
        "        return F.softmax(x, dim=-1)      \n",
        "        # dim=-1表示对最后一维作softmax计算，把取值范围限制在[0,1]，这里是每一行输出值的和应为1\n",
        "    else:\n",
        "        shape = x.shape\n",
        "        if valid_lens.dim() == 1:\n",
        "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])   # 如果你只是输入了一个维度的valid_lens显然要把它复制到每一行上\n",
        "            # 对于不同的样本也可以分别指定有效长度，这就是做repeat操作的目的\n",
        "        else:\n",
        "            valid_lens = valid_lens.reshape(-1)     # 不是1维的话，拉成1维的\n",
        "        # 在最后的轴上，被遮蔽的元素使⽤⼀个⾮常⼤的负值替换，从⽽其softmax (指数)输出为0\n",
        "        x = d2l.sequence_mask(x.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
        "        return F.softmax(x.reshape(shape), dim=-1)\n",
        "    \n",
        "def transpose_qkv(X, num_heads):\n",
        "    \"\"\"为了多注意⼒头的并⾏计算⽽变换形状\"\"\"\n",
        "    # 输⼊X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)\n",
        "    # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads, num_hiddens/num_heads)\n",
        "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1) \n",
        "    \n",
        "    # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数, num_hiddens/num_heads)\n",
        "    X = X.permute(0, 2, 1, 3) \n",
        "    \n",
        "    # 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数, num_hiddens/num_heads)\n",
        "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
        "\n",
        "def transpose_output(X, num_heads):\n",
        "    \"\"\"逆转transpose_qkv函数的操作\"\"\"\n",
        "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
        "    X = X.permute(0, 2, 1, 3)\n",
        "    return X.reshape(X.shape[0], X.shape[1], -1)\n",
        "\n",
        "class Multihead_attention(nn.Module):\n",
        "    def __init__(self, num_heads, dropout, query_size, key_size, value_size, \n",
        "    num_hiddens, bias=False, **kwargs):\n",
        "        super(Multihead_attention, self).__init__(**kwargs)\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = DotProd_Attention(dropout)\n",
        "        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\n",
        "        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\n",
        "        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\n",
        "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
        "    def forward(self, queries, keys, values, valid_lens):\n",
        "        # queries，keys，values的形状:\n",
        "        # (batch_size，查询或者“键－值”对的个数，num_hiddens)\n",
        "        # valid_lens 的形状:\n",
        "        # (batch_size，)或(batch_size，查询的个数) # 经过变换后，输出的queries，keys，values 的形状:\n",
        "        # (batch_size*num_heads，查询或者“键－值”对的个数，num_hiddens/num_heads)\n",
        "        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n",
        "        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n",
        "        values = transpose_qkv(self.W_v(values), self.num_heads)\n",
        "        \n",
        "        if valid_lens is not None: \n",
        "            # 在轴0，将第⼀项（标量或者⽮量）复制num_heads次，\n",
        "            # 然后如此复制第⼆项，然后诸如此类。\n",
        "            valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0)\n",
        "\n",
        "        # output的形状:(batch_size*num_heads，查询的个数，num_hiddens/num_heads)\n",
        "        output = self.attention(queries, keys, values, valid_lens)\n",
        "        # output_concat的形状:(batch_size，查询的个数，num_hiddens)\n",
        "        output_concat = transpose_output(output, self.num_heads)\n",
        "        return self.W_o(output_concat)\n",
        "\n",
        "# 基于位置的前馈网络（FFN，其实就是线性层，名字叫的好听点）\n",
        "class PositionWiseFFN(nn.Module):\n",
        "    \"\"\"基于位置的前馈⽹络\"\"\"\n",
        "    # 因为⽤同⼀个多层感知机对所有位置上的输⼊进⾏变换，所以当所有这些位置的输⼊相同时，它们的输出也是相同的\n",
        "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,\n",
        "    **kwargs):\n",
        "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
        "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
        "    def forward(self, X):\n",
        "        return self.dense2(self.relu(self.dense1(X)))\n",
        "# 测试下前馈网络\n",
        "# ffn = PositionWiseFFN(4, 4, 8)\n",
        "# ffn.eval()\n",
        "# print(ffn(torch.ones((2, 3, 4)))[0])  # 可以观察到同一位置的输出值相等\n",
        "\n",
        "# layerNorm和batchNorm的区别在于说，layernorm是针对于一个样本的所有特征来做归一化的，使得从一个样本上看过去是均值为0方差为1\n",
        "# 而batchnorm则是对当前一个batch内所有样本的同一列特征来做归一化，也就是说两者处理的维度不同\n",
        "\n",
        "# 残差连接与layernorm实现\n",
        "class AddNorm(nn.Module):\n",
        "    \"\"\"残差连接后进⾏层规范化\"\"\"\n",
        "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
        "        super(AddNorm, self).__init__(**kwargs)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.ln = nn.LayerNorm(normalized_shape)\n",
        "    def forward(self, X, Y):\n",
        "        return self.ln(self.dropout(Y) + X)\n",
        "# 测试下残差连接层，两个输入维度要一致\n",
        "# add_norm = AddNorm([3, 4], 0.5)\n",
        "# add_norm.eval()\n",
        "# print(add_norm(torch.ones((2, 3, 4)), torch.ones((2, 3, 4))).shape)\n",
        "\n",
        "# 编码器block的实现，transformer是要叠好几个encoderblock和decoderblock\n",
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"transformer编码器块\"\"\"\n",
        "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
        "    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
        "    dropout, use_bias=False, **kwargs):\n",
        "        super(EncoderBlock, self).__init__(**kwargs)\n",
        "        self.attention = Multihead_attention(\n",
        "            num_heads, dropout, query_size, key_size, value_size, \n",
        "            num_hiddens)\n",
        "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
        "        self.ffn = PositionWiseFFN(\n",
        "        ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
        "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
        "    def forward(self, X, valid_lens):\n",
        "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
        "        return self.addnorm2(Y, self.ffn(Y))\n",
        "# 可以看到，transformer编码器中的任何层都不会改变其输⼊的形状\n",
        "# 测试编码器block\n",
        "# X = torch.ones((2, 100, 24))\n",
        "# valid_lens = torch.tensor([3, 2])   # 分别设置两个样本的有效长度\n",
        "# encoder_blk = EncoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5)\n",
        "# encoder_blk.eval()\n",
        "# print(encoder_blk(X, valid_lens).shape)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"位置编码\"\"\"\n",
        "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # 创建⼀个⾜够⻓的P\n",
        "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
        "        X = torch.arange(max_len, dtype=torch.float32).reshape(\n",
        "        -1, 1) / torch.pow(10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
        "        self.P[:, :, 0::2] = torch.sin(X)\n",
        "        self.P[:, :, 1::2] = torch.cos(X)\n",
        "    def forward(self, X):\n",
        "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
        "        return self.dropout(X)\n",
        "\n",
        "# 叠加encoder_block\n",
        "# Transformer编码器输出的形状是（批量⼤⼩，时间步数⽬，num_hiddens）\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"transformer编码器\"\"\"\n",
        "    def __init__(self, vocab_size, key_size, query_size, value_size,\n",
        "    num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,\n",
        "    num_heads, num_layers, dropout, use_bias=False, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
        "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
        "        self.blks = nn.Sequential()\n",
        "        for i in range(num_layers):\n",
        "            self.blks.add_module(\"block\"+str(i),\n",
        "            EncoderBlock(key_size, query_size, value_size, num_hiddens,\n",
        "            norm_shape, ffn_num_input, ffn_num_hiddens,\n",
        "            num_heads, dropout, use_bias))\n",
        "    def forward(self, X, valid_lens):\n",
        "    # 因为位置编码值在-1和1之间，\n",
        "    # 因此嵌⼊值乘以嵌⼊维度的平⽅根进⾏缩放，\n",
        "    # 然后再与位置编码相加。\n",
        "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
        "        self.attention_weights = [None] * len(self.blks)\n",
        "        for i, blk in enumerate(self.blks):\n",
        "            X = blk(X, valid_lens)\n",
        "            self.attention_weights[i] = blk.attention.attention.attention_weights\n",
        "        return X\n",
        "\n",
        "# 测试Transformer_Encoder，两个block\n",
        "# encoder = TransformerEncoder(200, 24, 24, 24, 24, [100, 24], 24, 48, 8, 2, 0.5)\n",
        "# encoder.eval()\n",
        "# valid_lens = torch.tensor([3, 2])\n",
        "# print(encoder(torch.ones((2, 100), dtype=torch.long), valid_lens).shape)\n",
        "\n",
        "# transformer解码器也是由多个相同的层组成。在DecoderBlock类中实现的每个层包含了三个⼦层:\n",
        "# 解码器⾃注意⼒、“编码器-解码器”注意⼒和基于位置的前馈⽹络。这些⼦层也都被和紧随的layernorm围绕\n",
        "# 在掩蔽多头解码器⾃注意⼒层（第⼀个⼦层）中，查询、键和值都来⾃上⼀个解码器层的输出\n",
        "# 为了在解码器中保留⾃回归的属性，其掩蔽⾃注意⼒设定了参数dec_valid_lens，以便任何查询都只会与解码器中所有已经⽣成词元的位置（即直到该查询位置为⽌）进⾏注意⼒计算\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"解码器中第i个块\"\"\"\n",
        "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
        "    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
        "    dropout, i, **kwargs):\n",
        "        super(DecoderBlock, self).__init__(**kwargs)\n",
        "        self.i = i\n",
        "        \n",
        "        self.attention1 = Multihead_attention(num_heads, dropout, query_size, key_size, value_size, num_hiddens)\n",
        "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
        "        self.attention2 = Multihead_attention(num_heads, dropout, query_size, key_size, value_size, num_hiddens)\n",
        "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
        "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
        "        self.addnorm3 = AddNorm(norm_shape, dropout)\n",
        "    def forward(self, X, state):\n",
        "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
        "        # 训练阶段，输出序列的所有词元都在同⼀时间处理，\n",
        "        # 因此state[2][self.i]初始化为None。\n",
        "        # 预测阶段，输出序列是通过词元⼀个接着⼀个解码的，\n",
        "        # 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表⽰\n",
        "        if state[2][self.i] is None:\n",
        "            key_values = X\n",
        "        else:\n",
        "            key_values = torch.cat((state[2][self.i], X), axis=1)\n",
        "        state[2][self.i] = key_values\n",
        "        if self.training:\n",
        "            batch_size, num_steps, _ = X.shape\n",
        "            # dec_valid_lens的开头:(batch_size,num_steps),\n",
        "            # 其中每⼀⾏是[1,2,...,num_steps]\n",
        "            dec_valid_lens = torch.arange(\n",
        "                1, num_steps + 1, device=X.device).repeat(batch_size, 1)\n",
        "        else:\n",
        "            dec_valid_lens = None\n",
        "        # ⾃注意⼒\n",
        "        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)\n",
        "        Y = self.addnorm1(X, X2)\n",
        "        # 编码器－解码器注意⼒。\n",
        "        # enc_outputs的开头:(batch_size,num_steps,num_hiddens)\n",
        "        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\n",
        "        Z = self.addnorm2(Y, Y2)\n",
        "        return self.addnorm3(Z, self.ffn(Z)), state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 构建transformer解码器，还有后面全连接层输出\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, key_size, query_size, value_size,\n",
        "                num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,\n",
        "                num_heads, num_layers, dropout, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
        "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
        "        self.blks = nn.Sequential()\n",
        "        for i in range(num_layers):\n",
        "            self.blks.add_module(\"block\"+str(i),\n",
        "                    DecoderBlock(key_size, query_size, value_size, num_hiddens, \n",
        "                    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i))\n",
        "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
        "\n",
        "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
        "        return [enc_outputs, enc_valid_lens, [None] * self.num_layers]\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
        "        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]\n",
        "        for i, blk in enumerate(self.blks):\n",
        "            X, state = blk(X, state)\n",
        "            # 解码器⾃注意⼒权重\n",
        "            self._attention_weights[0][i] = blk.attention1.attention.attention_weights\n",
        "            # “编码器－解码器”⾃注意⼒权重\n",
        "            self._attention_weights[1][i] = blk.attention2.attention.attention_weights\n",
        "        return self.dense(X), state\n",
        "\n",
        "    @property\n",
        "    def attention_weights(self):\n",
        "        return self._attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx0vTZz8u4cf"
      },
      "source": [
        "## 自写学习版"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKyoePgFtkZx",
        "outputId": "7d551c58-16f7-445e-b13c-8933ac2e331c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[7, 1, 0, 0],\n",
            "        [6, 2, 1, 3]])\n",
            "Parameter containing:\n",
            "tensor([[-0.1941,  0.3929, -0.7884,  1.0630,  0.2310, -0.6385, -0.3659, -0.6311],\n",
            "        [ 0.0708,  0.6229,  0.0506,  0.3370, -1.7553, -1.0314,  0.5341,  1.8713],\n",
            "        [ 0.2468,  0.4240, -1.3750, -0.0041,  0.1440,  0.4418,  1.4496, -0.8339],\n",
            "        [ 0.1772, -0.6893, -0.5072,  1.7150, -0.9951, -1.5135, -1.0859,  0.1859],\n",
            "        [ 0.1687, -0.6958, -0.5035,  1.1720, -0.0776,  1.0870, -0.3335,  0.0664],\n",
            "        [-1.1862, -0.9110, -1.5377,  1.8045, -0.2976,  2.1323, -0.4915,  0.7331],\n",
            "        [-1.4168, -2.2570, -2.2182, -0.0821,  0.4044,  0.9650,  1.1716,  0.7415],\n",
            "        [ 0.0207, -0.1584,  1.1953,  0.9947, -0.6578, -1.7685,  1.2537,  0.9954],\n",
            "        [ 0.1161, -0.2006, -0.9234,  0.2654, -0.7097,  0.4820,  0.5363,  0.4558]],\n",
            "       requires_grad=True)\n",
            "tensor([[[ 0.0207, -0.1584,  1.1953,  0.9947, -0.6578, -1.7685,  1.2537,\n",
            "           0.9954],\n",
            "         [ 0.0708,  0.6229,  0.0506,  0.3370, -1.7553, -1.0314,  0.5341,\n",
            "           1.8713],\n",
            "         [-0.1941,  0.3929, -0.7884,  1.0630,  0.2310, -0.6385, -0.3659,\n",
            "          -0.6311],\n",
            "         [-0.1941,  0.3929, -0.7884,  1.0630,  0.2310, -0.6385, -0.3659,\n",
            "          -0.6311]],\n",
            "\n",
            "        [[-1.4168, -2.2570, -2.2182, -0.0821,  0.4044,  0.9650,  1.1716,\n",
            "           0.7415],\n",
            "         [ 0.2468,  0.4240, -1.3750, -0.0041,  0.1440,  0.4418,  1.4496,\n",
            "          -0.8339],\n",
            "         [ 0.0708,  0.6229,  0.0506,  0.3370, -1.7553, -1.0314,  0.5341,\n",
            "           1.8713],\n",
            "         [ 0.1772, -0.6893, -0.5072,  1.7150, -0.9951, -1.5135, -1.0859,\n",
            "           0.1859]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Miniconda3\\envs\\pytorch_basic\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as ny\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# 关于word embedding, 以序列建模为例\n",
        "# 考虑source_sentence和target_sentence\n",
        "# 构建序列，序列的字符以其在词表中的索引的形式表示\n",
        "\n",
        "# 定义下单词表的最大长度\n",
        "max_num_src_words = 8    # 意味着最多一个句子有8个单词组成\n",
        "max_num_tgt_words = 8\n",
        "model_dim = 8    \n",
        "# 在论文中维度是512，这个为了测试方便写小点，相当于输出模型的特征维度\n",
        "\n",
        "src_len = torch.Tensor([2, 4]).to(torch.int32)\n",
        "tgt_len = torch.Tensor([4, 3]).to(torch.int32)\n",
        "\n",
        "# 定义序列的最大长度\n",
        "max_src_seq_len = 5\n",
        "max_tgt_seq_len = 5\n",
        "max_pos_len = 5   \n",
        "# 位置编码的长度显然要跟序列长度一致，因为实际上就是矩阵加法，维度要一致\n",
        "\n",
        "\n",
        "# 单词索引构成的句子，我们需要做一个padding操作，把整序列的长度是一致的，空余地方补0\n",
        "# 同时把形式构造成能够进行小批量训练的形式\n",
        "src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_src_words, (L,)), (0, max(src_len) - L)), 0)  \n",
        "                            for L in src_len])\n",
        "tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_tgt_words, (L,)), (0, max(tgt_len) - L)), 0)  \n",
        "                            for L in tgt_len])\n",
        "\n",
        "# 构造embedding，对于词向量来说必不可少，包括在ViT模型中处理图像，个人理解相当于一个编码的过程\n",
        "# 利用nn.Embedding类，相当于对句子中的每一个单词做一个编码索引，事实上就是把前面seq的值当做index\n",
        "src_embedding_table = nn.Embedding(max_num_src_words+1, model_dim)   # 这里要加1是因为要把第0个位置让个padding的做索引\n",
        "tgt_embedding_table = nn.Embedding(max_num_tgt_words+1, model_dim)\n",
        "src_embedding = src_embedding_table(src_seq)   # 调用forward方法传入句子，得到embedding结果\n",
        "tgt_embedding = tgt_embedding_table(tgt_seq)\n",
        "\n",
        "print(src_seq)\n",
        "print(src_embedding_table.weight)\n",
        "print(src_embedding)    # 打印一下可以直观的发现编码规律\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn1qu6nVno05",
        "outputId": "3651ac93-6dfa-4546-b38d-b0cce284938a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
            "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
            "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
            "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
            "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
            "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
            "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
            "           9.9955e-01,  3.0000e-03,  1.0000e+00]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
            "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
            "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
            "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
            "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
            "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
            "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
            "           9.9955e-01,  3.0000e-03,  1.0000e+00]]])\n",
            "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
            "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
            "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
            "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
            "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
            "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
            "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
            "           9.9955e-01,  3.0000e-03,  1.0000e+00]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
            "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
            "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
            "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
            "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
            "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
            "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
            "           9.9955e-01,  3.0000e-03,  1.0000e+00]]])\n"
          ]
        }
      ],
      "source": [
        "# 构造position embedding 这里用到了pytorch矩阵相乘的广播机制，具体公式看论文\n",
        "# 其中公式中pos代表行，i代表列即词向量的维度  位置编码，保证模型在深度加深时不会丢失位置信息\n",
        "# 实际上在ViT中也要位置编码，不管是NLP还是CV我们都要保证前后语义的关联信息是不变的\n",
        "\n",
        "pos_mat = torch.arange(max_pos_len).reshape(-1,1)\n",
        "i_mat = torch.pow(10000, torch.arange(0, model_dim, 2).reshape(1,-1) / model_dim)\n",
        "pos_embedding_table = torch.zeros(max_pos_len, model_dim)\n",
        "pos_embedding_table[:, 0::2] = torch.sin(pos_mat / i_mat)  # 偶数列用sin\n",
        "pos_embedding_table[:, 1::2] = torch.cos(pos_mat / i_mat)  # 奇数列用cos\n",
        "\n",
        "pe_embedding = nn.Embedding(max_pos_len, model_dim)   # 把table传入Embedding类构建层\n",
        "pe_embedding.weight = nn.Parameter(pos_embedding_table, requires_grad=False)\n",
        "\n",
        "src_pos = torch.cat([torch.unsqueeze(torch.arange(max(src_len)),0) for _ in src_len]).to(torch.int32)\n",
        "tgt_pos = torch.cat([torch.unsqueeze(torch.arange(max(tgt_len)),0) for _ in tgt_len]).to(torch.int32)\n",
        "\n",
        "src_pe_embedding = pe_embedding(src_pos)\n",
        "tgt_pe_embedding = pe_embedding(tgt_pos)\n",
        "print(src_pe_embedding)\n",
        "print(tgt_pe_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Masked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhU760210TcW",
        "outputId": "e01cf473-c6b4-4f04-df0e-66b8b38d44da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-6.6827e-01, -6.2055e-01, -1.0000e+09, -1.0000e+09],\n",
            "         [-1.6404e-01,  5.9722e-01, -1.0000e+09, -1.0000e+09],\n",
            "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
            "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "        [[-2.3849e-01,  5.4275e-01, -5.8056e-01,  3.5201e-02],\n",
            "         [-8.8779e-01, -3.4283e-02,  7.3794e-01, -1.0497e+00],\n",
            "         [ 6.6987e-01,  9.8241e-01, -3.8788e-01,  1.8632e+00],\n",
            "         [ 1.3347e+00, -1.6914e+00, -4.0881e-01,  5.8134e-01]]])\n",
            "tensor([[[0.4881, 0.5119, 0.0000, 0.0000],\n",
            "         [0.3184, 0.6816, 0.0000, 0.0000],\n",
            "         [0.2500, 0.2500, 0.2500, 0.2500],\n",
            "         [0.2500, 0.2500, 0.2500, 0.2500]],\n",
            "\n",
            "        [[0.1920, 0.4193, 0.1364, 0.2524],\n",
            "         [0.1078, 0.2530, 0.5476, 0.0916],\n",
            "         [0.1663, 0.2274, 0.0578, 0.5486],\n",
            "         [0.5902, 0.0286, 0.1032, 0.2779]]])\n"
          ]
        }
      ],
      "source": [
        "# 构造encoder中的self-attention mask\n",
        "\n",
        "# 先得到有效矩阵\n",
        "valid_encode_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), (0, max(src_len)-L)), 0) for L in src_len]), 2) \n",
        "# (0, max(src_len)-L))表示左侧padding0列，右侧padding max(src_len)-L)列\n",
        "# 由于在注意力公式里有Q*K的转置实际上会得到一个k*k的方阵，因此我们这里先构造出一个邻接矩阵即得到要进行softmax的位置，值为1\n",
        "valid_matrix = torch.bmm(valid_encode_pos, valid_encode_pos.transpose(1,2))\n",
        "invalid_matrix = 1- valid_matrix     # 1减得到无效区域\n",
        "invalid_matrix = invalid_matrix.to(torch.bool)   # 转成bool类型方便后面利用mask_filled API，值为True的位置即需要mask处理\n",
        "\n",
        "score = torch.randn(2, max(src_len), max(src_len))   # 构造一个矩阵测试一下\n",
        "masked_score = score.masked_fill(invalid_matrix, -1e9)  # 需要mask的位置给它赋一个负无穷的数，这样经过softmax之后就为0\n",
        "prob = F.softmax(masked_score, -1)\n",
        "print(masked_score)\n",
        "print(prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[False,  True,  True,  True],\n",
            "         [False, False,  True,  True],\n",
            "         [False, False, False,  True],\n",
            "         [False, False, False, False]],\n",
            "\n",
            "        [[False,  True,  True,  True],\n",
            "         [False, False,  True,  True],\n",
            "         [False, False, False,  True],\n",
            "         [ True,  True,  True,  True]]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.7647, 0.2353, 0.0000, 0.0000],\n",
              "         [0.8152, 0.1500, 0.0348, 0.0000],\n",
              "         [0.5327, 0.2886, 0.0371, 0.1417]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.8665, 0.1335, 0.0000, 0.0000],\n",
              "         [0.3606, 0.4226, 0.2168, 0.0000],\n",
              "         [0.2500, 0.2500, 0.2500, 0.2500]]])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 构造decoder中的self-attention mask\n",
        "# 因为目标序列训练的时候是已知的，为了保证预测的合理性，所以我们要把当前预测单词之后的序列遮住\n",
        "# 因此需要在attention之前做masked处理\n",
        "# (0, max(tgt_len)-L, 0, max(tgt_len)-L)   分别表示上、下、左、右要不要padding\n",
        "valid_decoder_tri_matrix = torch.cat([torch.unsqueeze(F.pad(torch.tril(torch.ones(L, L)), (0, max(tgt_len)-L, 0, max(tgt_len)-L)), 0) for L in tgt_len])    # 先生成上三角矩阵，很直观的符合我们需求，1表示当前预测位置，每次都只传入之前的单词\n",
        "# 同时我们要把每个batch的样本pad成维度一致的形状，不然显然无法做矩阵运算\n",
        "# 后面的步骤几乎和上一个masked一模一样了\n",
        "invalid_decoder_tri_matrix = 1 - valid_decoder_tri_matrix\n",
        "invalid_decoder_tri_matrix = invalid_decoder_tri_matrix.to(torch.bool)\n",
        "print(invalid_decoder_tri_matrix)\n",
        "score = torch.randn(2, max(tgt_len), max(tgt_len))\n",
        "masked_score = score.masked_fill(invalid_decoder_tri_matrix, -1e9)\n",
        "F.softmax(masked_score, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('pytorch_basic')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "5d7f9c8d1ba637fbaa165b7e1b068b277a24bdbbae2fecef2adf18f32dc564ea"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1773, -0.1150, -0.5396],\n",
      "         [ 0.2456, -0.4637, -0.8172],\n",
      "         [ 0.7823, -0.7720, -0.8598]],\n",
      "\n",
      "        [[ 0.4552,  0.3404,  0.0755],\n",
      "         [ 0.5677,  0.2616, -0.0390],\n",
      "         [ 0.8455, -0.7918, -0.8138]]], grad_fn=<TransposeBackward1>)\n",
      "tensor([[[ 0.7823, -0.7720, -0.8598],\n",
      "         [ 0.8455, -0.7918, -0.8138]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "bs, T = 2, 3   # 批大小，输入序列长度\n",
    "input_size, hidden_size = 2, 3 # 输入特征大小，隐含层特征大小\n",
    "input  = torch.randn(bs, T, input_size)    # 随机初始化一个输入特征序列\n",
    "h_prev = torch.zeros(bs, hidden_size)     # 初始隐含状态\n",
    "\n",
    "# step1 调用pytorch RNN API\n",
    "rnn = nn.RNN(input_size, hidden_size, batch_first = True)\n",
    "rnn_output, state_final = rnn(input, h_prev.unsqueeze(0))\n",
    "print(rnn_output)\n",
    "print(state_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单向单层RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1773, -0.1150, -0.5396],\n",
      "         [ 0.2456, -0.4637, -0.8172],\n",
      "         [ 0.7823, -0.7720, -0.8598]],\n",
      "\n",
      "        [[ 0.4552,  0.3404,  0.0755],\n",
      "         [ 0.5677,  0.2616, -0.0390],\n",
      "         [ 0.8455, -0.7918, -0.8138]]], grad_fn=<CopySlices>)\n",
      "tensor([[[ 0.7823, -0.7720, -0.8598],\n",
      "         [ 0.8455, -0.7918, -0.8138]]], grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# step2 手写一个rnn_forward函数，实现RNN的计算原理（计算公式可以看pytorch官方文档）\n",
    "# ht = tanh(Wix*Xt + bix + Whh*h(t-1) + bhh)\n",
    "def rnn_forward(input, weight_ix, weight_h, bias_ix, bias_h, h_prev):\n",
    "    bs, T, input_size = input.shape\n",
    "    h_dim = weight_ix.shape[0]\n",
    "    h_out = torch.zeros(bs, T, h_dim)     # 初始化一个输出（状态）矩阵\n",
    "\n",
    "    for t in range(T):\n",
    "        x = input[:, t, :].unsqueeze(2)       # 当前时刻的输入特征， bs*input_size*1\n",
    "        w_ix_batch = weight_ix.unsqueeze(0).tile(bs, 1, 1)    # 先升1维然后再复制到每一个batch上，因为初始weight_ix一般是一个二维矩阵\n",
    "        w_hh_batch = weight_h.unsqueeze(0).tile(bs, 1, 1)\n",
    "\n",
    "        w_times_x = torch.bmm(w_ix_batch, x).squeeze(-1)    # bs*h_dim\n",
    "        w_times_h = torch.bmm(w_hh_batch, h_prev.unsqueeze(2)).squeeze(-1)\n",
    "\n",
    "        h_prev = torch.tanh(w_times_x+bias_ix+w_times_h+bias_h)\n",
    "        h_out[:,t,:] = h_prev\n",
    "    \n",
    "    return h_out, h_prev.unsqueeze(0)\n",
    "\n",
    "custom_rnn_output, custom_state_final = rnn_forward(input, rnn.weight_ih_l0, rnn.weight_hh_l0, rnn.bias_ih_l0, rnn.bias_hh_l0, h_prev)\n",
    "print(custom_rnn_output )\n",
    "print(custom_state_final )\n",
    "# 发现结果和官方API一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 双向RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def birectional_rnn_forward(input,weight_ih,weight_hh,bias_ih,bias_hh,h_prev,\n",
    "        weight_ih_reverse,weight_hh_reverse,bias_ih_reverse,bias_hh_reverse,h_prev_reverse):\n",
    "    \n",
    "    bs,T,input_size = input.size()\n",
    "    h_dim = weight_ih.shape[0]       #这个维度是根据公式来判断的\n",
    "    h_out = torch.zeros(bs,T,h_dim*2)  #初始化一个输出(状态)矩阵,注意这边是双向的结构，所以要乘以2\n",
    "\n",
    "    forward_output = rnn_forward(input,weight_ih,weight_hh,bias_ih,bias_hh,h_prev)[0]   #2*3*3\n",
    "    backward_output = rnn_forward(torch.flip(input,[1]),weight_ih_reverse,weight_hh_reverse,bias_ih_reverse,bias_hh_reverse,h_prev_reverse)[0]\n",
    "\n",
    "    h_out[:,:,:h_dim] = forward_output\n",
    "    h_out[:,:,h_dim:] = torch.flip(backward_output,[1])\n",
    "\n",
    "    h_n= torch.zeros(bs,2,h_dim) \n",
    "    h_n[:,0,:] = forward_output[:,-1,:]\n",
    "    h_n[:,1,:] = backward_output[:,-1,:]\n",
    "\n",
    "    h_n = h_n.transpose(0,1)    \n",
    "    return h_out,h_n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch_basic')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d7f9c8d1ba637fbaa165b7e1b068b277a24bdbbae2fecef2adf18f32dc564ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
